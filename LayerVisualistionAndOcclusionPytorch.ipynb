{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhiyantaabhishek/cs231n_2019/blob/master/LayerVisualistionAndOcclusionPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6AshfGxaMDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qckfRCckaQGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "#import datasets in torchvision\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "#import model zoo in torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POyPQpKwaSXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/Niranjankumar-c/DeepLearning-PadhAI"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3O4ajxWaVD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd DeepLearning-PadhAI/DeepLearning_Materials/6_VisualizationCNN_Pytorch/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-giNoKCaXOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reading the labels of data we uploaded\n",
        "with open(\"data/imagenet_labels.txt\") as f:\n",
        "    classes = eval(f.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVT8EzAsaZjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDlMikX_agYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining the transformations for the data\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    #normalize the images with imagenet data mean and std\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsLOhMotajSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the data we uploaded as evaluation data and apply the transformations\n",
        "\n",
        "evalset = torchvision.datasets.ImageFolder(root = \"./data/imagenet\", transform = transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktEy-7PTamT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a data loader for evaluation\n",
        "\n",
        "batch_size=1 #batch size\n",
        "evalloader = torch.utils.data.DataLoader(evalset, batch_size = batch_size, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgmhx_KWar88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#looking at data using iter\n",
        "\n",
        "dataiter = iter(evalloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "#shape of images bunch\n",
        "print(images.shape)\n",
        "#shape of single image in a bunch\n",
        "print(images[0].shape)\n",
        "\n",
        "#label of the image\n",
        "print(labels[0].item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgzdnVRQavID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(images[0,2,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps75HDP-axzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for visualization we will use vgg16 pretrained on imagenet data\n",
        "\n",
        "model = models.vgg16(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4mVGyY2a0Qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcbfXaYOa4mC",
        "colab_type": "text"
      },
      "source": [
        "## Visualise image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDRf0VJQa6qe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img, title):\n",
        "  \n",
        "  \"\"\"Custom function to display the image using matplotlib\"\"\"\n",
        "  \n",
        "  #define std correction to be made\n",
        "  std_correction = np.asarray([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n",
        "  \n",
        "  #define mean correction to be made\n",
        "  mean_correction = np.asarray([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
        "  \n",
        "  #convert the tensor img to numpy img and de normalize \n",
        "  npimg = np.multiply(img.numpy(), std_correction) + mean_correction\n",
        "  \n",
        "  #plot the numpy image\n",
        "  plt.figure(figsize = (batch_size * 4, 4))\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UODaX-ya9dK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#custom function to fetch images from dataloader\n",
        "\n",
        "def show_batch_images(dataloader):\n",
        "  images,_ = next(iter(dataloader))\n",
        "  \n",
        "  #run the model on the images\n",
        "  outputs = model(images)\n",
        "  \n",
        "  #get the maximum class \n",
        "  _, pred = torch.max(outputs.data, 1)\n",
        "  \n",
        "  #make grid\n",
        "  img = torchvision.utils.make_grid(images)\n",
        "  \n",
        "  #call the function\n",
        "  imshow(img, title=[classes[x.item()] for x in pred])\n",
        "  \n",
        "  return images, pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfg3o11AbAS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, pred = show_batch_images(evalloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ecixL0ebDSK",
        "colab_type": "text"
      },
      "source": [
        "## Occlusion analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNwS_qy4bHHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#running inference on the images without occlusion\n",
        "\n",
        "#vgg16 pretrained model\n",
        "outputs = model(images)\n",
        "print(outputs.shape)\n",
        "\n",
        "#passing the outputs through softmax to interpret them as probability\n",
        "outputs = nn.functional.softmax(outputs, dim = 1)\n",
        "\n",
        "#getting the maximum predicted label\n",
        "prob_no_occ, pred = torch.max(outputs.data, 1)\n",
        "\n",
        "#get the first item\n",
        "prob_no_occ = prob_no_occ[0].item()\n",
        "\n",
        "print(prob_no_occ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXM63WZtbN5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#custom function to conduct occlusion experiments\n",
        "\n",
        "def occlusion(model, image, label, occ_size = 50, occ_stride = 50, occ_pixel = 0.5):\n",
        "  \n",
        "    #get the width and height of the image\n",
        "    width, height = image.shape[-2], image.shape[-1]\n",
        "  \n",
        "    #setting the output image width and height\n",
        "    output_height = int(np.ceil((height-occ_size)/occ_stride))\n",
        "    output_width = int(np.ceil((width-occ_size)/occ_stride))\n",
        "  \n",
        "    #create a white image of sizes we defined\n",
        "    heatmap = torch.zeros((output_height, output_width))\n",
        "    \n",
        "    #iterate all the pixels in each column\n",
        "    for h in range(0, height):\n",
        "        for w in range(0, width):\n",
        "            \n",
        "            h_start = h*occ_stride\n",
        "            w_start = w*occ_stride\n",
        "            h_end = min(height, h_start + occ_size)\n",
        "            w_end = min(width, w_start + occ_size)\n",
        "            \n",
        "            if (w_end) >= width or (h_end) >= height:\n",
        "                continue\n",
        "            \n",
        "            input_image = image.clone().detach()\n",
        "            \n",
        "            #replacing all the pixel information in the image with occ_pixel(grey) in the specified location\n",
        "            input_image[:, :, w_start:w_end, h_start:h_end] = occ_pixel\n",
        "            \n",
        "            #run inference on modified image\n",
        "            output = model(input_image)\n",
        "            output = nn.functional.softmax(output, dim=1)\n",
        "            prob = output.tolist()[0][label]\n",
        "            \n",
        "            #setting the heatmap location to probability value\n",
        "            heatmap[h, w] = prob \n",
        "\n",
        "    return heatmap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI_Q_5EvbSF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "heatmap = occlusion(model, images, pred[0].item(), 32, 14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeoEJg92bUqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#displaying the image using seaborn heatmap and also setting the maximum value of gradient to probability\n",
        "imgplot = sns.heatmap(heatmap, xticklabels=False, yticklabels=False, vmax=prob_no_occ)\n",
        "figure = imgplot.get_figure()    \n",
        "figure.savefig('svm_conf.png', dpi=400)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ-6HTAhbXCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for filter visualization, we will use alexnet pretrained with imagenet data\n",
        "\n",
        "alexnet = models.alexnet(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73cSOShmbaco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#structure of the alexnet\n",
        "print(alexnet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC6RKz1zbdwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_filters_single_channel_big(t):\n",
        "    \n",
        "    #setting the rows and columns\n",
        "    nrows = t.shape[0]*t.shape[2]\n",
        "    ncols = t.shape[1]*t.shape[3]\n",
        "    \n",
        "    \n",
        "    npimg = np.array(t.numpy(), np.float32)\n",
        "    npimg = npimg.transpose((0, 2, 1, 3))\n",
        "    npimg = npimg.ravel().reshape(nrows, ncols)\n",
        "    \n",
        "    npimg = npimg.T\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(ncols/10, nrows/200))    \n",
        "    imgplot = sns.heatmap(npimg, xticklabels=False, yticklabels=False, cmap='gray', ax=ax, cbar=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmTpOkKwbjWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_filters_single_channel(t):\n",
        "    \n",
        "    #kernels depth * number of kernels\n",
        "    nplots = t.shape[0]*t.shape[1]\n",
        "    ncols = 12\n",
        "    \n",
        "    nrows = 1 + nplots//ncols\n",
        "    #convert tensor to numpy image\n",
        "    npimg = np.array(t.numpy(), np.float32)\n",
        "    \n",
        "    count = 0\n",
        "    fig = plt.figure(figsize=(ncols, nrows))\n",
        "    \n",
        "    #looping through all the kernels in each channel\n",
        "    for i in range(t.shape[0]):\n",
        "        for j in range(t.shape[1]):\n",
        "            count += 1\n",
        "            ax1 = fig.add_subplot(nrows, ncols, count)\n",
        "            npimg = np.array(t[i, j].numpy(), np.float32)\n",
        "            npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "            npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "            ax1.imshow(npimg)\n",
        "            ax1.set_title(str(i) + ',' + str(j))\n",
        "            ax1.axis('off')\n",
        "            ax1.set_xticklabels([])\n",
        "            ax1.set_yticklabels([])\n",
        "   \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zojw7DQSboTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_filters_multi_channel(t):\n",
        "    \n",
        "    #get the number of kernals\n",
        "    num_kernels = t.shape[0]    \n",
        "    \n",
        "    #define number of columns for subplots\n",
        "    num_cols = 12\n",
        "    #rows = num of kernels\n",
        "    num_rows = num_kernels\n",
        "    \n",
        "    #set the figure size\n",
        "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
        "    \n",
        "    #looping through all the kernels\n",
        "    for i in range(t.shape[0]):\n",
        "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
        "        \n",
        "        #for each kernel, we convert the tensor to numpy \n",
        "        npimg = np.array(t[i].numpy(), np.float32)\n",
        "        #standardize the numpy image\n",
        "        npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "        npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "        npimg = npimg.transpose((1, 2, 0))\n",
        "        ax1.imshow(npimg)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title(str(i))\n",
        "        ax1.set_xticklabels([])\n",
        "        ax1.set_yticklabels([])\n",
        "        \n",
        "    plt.savefig('myimage.png', dpi=100)    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGllsPWwbraO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_weights(model, layer_num, single_channel = True, collated = False):\n",
        "  \n",
        "  #extracting the model features at the particular layer number\n",
        "  layer = model.features[layer_num]\n",
        "  \n",
        "  #checking whether the layer is convolution layer or not \n",
        "  if isinstance(layer, nn.Conv2d):\n",
        "    #getting the weight tensor data\n",
        "    weight_tensor = model.features[layer_num].weight.data\n",
        "    \n",
        "    if single_channel:\n",
        "      if collated:\n",
        "        plot_filters_single_channel_big(weight_tensor)\n",
        "      else:\n",
        "        plot_filters_single_channel(weight_tensor)\n",
        "        \n",
        "    else:\n",
        "      if weight_tensor.shape[1] == 3:\n",
        "        plot_filters_multi_channel(weight_tensor)\n",
        "      else:\n",
        "        print(\"Can only plot weights with three channels with single channel = False\")\n",
        "        \n",
        "  else:\n",
        "    print(\"Can only visualize layers which are convolutional\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpWnlOqlbuLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#visualize weights for alexnet - first conv layer\n",
        "\n",
        "plot_weights(alexnet, 0, single_channel = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruyc5xenbwir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plotting single channel images\n",
        "\n",
        "plot_weights(alexnet, 0, single_channel = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9amFMJLbzOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plot for 3rd layer -> 2nd conv layer\n",
        "plot_weights(alexnet, 3, single_channel = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuS4pQ1rb1Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " plot_weights(alexnet, 0, single_channel = True, collated = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gFLAR6Ob3g9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_weights(alexnet, 3, single_channel = True, collated = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L526hgkhb5mY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_weights(alexnet, 6, single_channel = True, collated = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5ZypOX4b7dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for vgg16\n",
        "\n",
        "plot_weights(model, 0, single_channel = True, collated = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR9-UE1gb9eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_weights(model, 2, single_channel = True, collated = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4syp2Gw0b_ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_weights(model, 5, single_channel = True, collated = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBzWFqMWcBi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_weights(model, 0, single_channel = False, collated = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}